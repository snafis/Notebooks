{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "pytorch_notebooks",
   "display_name": "pytorch_notebooks"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pytorch-logo.jpg\" width=\"400\" align=\"center\">\n",
    "\n",
    "# Welcome to PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/andrej_karpathy_on_pytorch.png\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the instruction guide here to install PyTorch locally:\n",
    "- https://pytorch.org/get-started/locally/\n",
    "\n",
    "Alternatively, use Google Colab Notebook here:\n",
    "- https://colab.research.google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local installation and versions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.4.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'0.5.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.18.1'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch + GPU/TPU in Google's Colab\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colaboratory is a Google research project created to help disseminate machine learning education and research. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n",
    "\n",
    "Colaboratory notebooks are stored in Google Drive and can be shared just as you would with Google Docs or Sheets. Colaboratory is free to use.\n",
    " -- https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "\n",
    "**Setup**\n",
    "- Go to https://colab.research.google.com\n",
    "- Create a new python 3 notebook\n",
    "- Enable the GPU: \"Edit -> Notebook settings -> Hardware accelerator: GPU -> Save\"\n",
    "- Then try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1.4.0\ncpu\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, what is PyTorch exactly?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"PyTorch - From Research To Production\n",
    "\n",
    "An open source deep learning platform that provides a seamless path from research prototyping to production deployment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dynamic_graph.gif\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors for Deep Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs, outputs, and transformations within neural networks are all represented using **tensors**, and as a result, neural network programming utilises tensors heavily.\n",
    "\n",
    "> A tensor is the primary data structure used by neural networks.\n",
    "\n",
    "The concept of a tensor is a mathematical generalisation of other more specific concepts - scalars, vectors and matrices. Let’s look at some specific instances of those concepts to develop the intution for tensor generalisation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/scalar-vector-matrix-tensor.jpeg\" width=\"400\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "scalar: \n s = 2\n"
    }
   ],
   "source": [
    "# scalar\n",
    "s = 2\n",
    "print(\"scalar: \\n s =\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "vector: \n v = [1 2 3 4]\n1\n"
    }
   ],
   "source": [
    "# vector\n",
    "v = [1, 2, 3, 4]\n",
    "print(\"vector: \\n v =\", np.array(v))\n",
    "## accessing first element\n",
    "print(v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "matrix: \n m = [[ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]]\n12\n"
    }
   ],
   "source": [
    "# matrix\n",
    "m = [[1, 4, 5, 12], [-5, 8, 9, 0],[-6, 7, 11, 19]]\n",
    "print(\"matrix: \\n m =\", np.array(m))\n",
    "## accessing last element of the first list\n",
    "print(m[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor: \n t = [[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]\n\n [[19 20 21]\n  [22 23 24]\n  [25 26 27]]]\n21\n"
    }
   ],
   "source": [
    "# tensor (nd array)\n",
    "t = [[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]\n",
    "print(\"tensor: \\n t =\", np.array(t))\n",
    "## accessing an element\n",
    "print(t[2][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexes required to access an element**\n",
    "\n",
    "The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure.\n",
    "\n",
    "\n",
    "> data structure => indexes requires\n",
    "\n",
    "> * scalar => 0\n",
    "> * vector => 1\n",
    "> * matrix => 2\n",
    "> * ndarray => n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor in PyTorch\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uninitialised**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "scalar:  tensor([0.])\nvector:  tensor([7.0065e-45, 0.0000e+00, 2.3694e-38])\nmatrix:  tensor([[ 1.1210e-44,  0.0000e+00,  0.0000e+00,  1.8367e-40,  3.9427e+04],\n        [-2.0005e+00,  3.9440e+04, -1.5849e+29,  4.2039e-45,  0.0000e+00],\n        [ 0.0000e+00,  1.4013e-45,  3.9440e+04, -2.0005e+00,  3.9427e+04]])\ntensor:  tensor([[[ 0.0000e+00, -8.5899e+09,  5.4839e+04],\n         [ 8.5920e+09,  2.2421e-44,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n\n        [[ 0.0000e+00,  2.8217e+32,  4.5879e-41],\n         [ 2.8217e+32,  4.5879e-41,  2.8217e+32],\n         [ 4.5879e-41,  4.9045e-44,  0.0000e+00],\n         [ 7.0065e-45,  0.0000e+00,  0.0000e+00],\n         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n\n        [[ 0.0000e+00,  7.3755e-40,  2.8217e+32],\n         [ 4.5879e-41,  2.8217e+32,  4.5879e-41],\n         [ 2.8217e+32,  4.5879e-41,  1.4013e-45],\n         [ 0.0000e+00,  1.4013e-45,  0.0000e+00],\n         [ 1.5863e+30,  4.5879e-41,  1.5863e+30]]])\n"
    }
   ],
   "source": [
    "# scalar/1 element\n",
    "x = torch.empty(1)\n",
    "print(\"scalar: \", x)\n",
    "# vector\n",
    "x = torch.empty(3)\n",
    "print(\"vector: \", x)\n",
    "# matrix\n",
    "x = torch.empty(3, 5)\n",
    "print(\"matrix: \", x)\n",
    "# tensor\n",
    "x = torch.empty(3, 5, 3)\n",
    "print(\"tensor: \", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filled with zeros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "scalar:  tensor([0], dtype=torch.int32)\nvector:  tensor([0, 0, 0], dtype=torch.int32)\nmatrix:  tensor([[0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0]], dtype=torch.int32)\ntensor:  tensor([[[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]]], dtype=torch.int32)\n"
    }
   ],
   "source": [
    "# scalar/1 element\n",
    "x = torch.zeros(1, dtype=torch.int)\n",
    "print(\"scalar: \", x)\n",
    "# vector\n",
    "x = torch.zeros(3, dtype=torch.int)\n",
    "print(\"vector: \", x)\n",
    "# matrix\n",
    "x = torch.zeros(3, 5, dtype=torch.int)\n",
    "print(\"matrix: \", x)\n",
    "# tensor\n",
    "x = torch.zeros(3, 5, 3, dtype=torch.int)\n",
    "print(\"tensor: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomly initialised and of double data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "scalar:  tensor([0.3055], dtype=torch.float64)\nvector:  tensor([0.2178, 0.4272, 0.5825], dtype=torch.float64)\nmatrix:  tensor([[0.9853, 0.1155, 0.0419, 0.1673, 0.9414],\n        [0.5901, 0.8637, 0.1335, 0.5174, 0.4313],\n        [0.5631, 0.1608, 0.8215, 0.2788, 0.6948]], dtype=torch.float64)\ntensor:  tensor([[[0.7289, 0.3684, 0.0513],\n         [0.0770, 0.0869, 0.9821],\n         [0.4691, 0.0084, 0.4949],\n         [0.0963, 0.7806, 0.6972],\n         [0.9902, 0.9085, 0.5838]],\n\n        [[0.8757, 0.3817, 0.1414],\n         [0.3053, 0.9752, 0.3242],\n         [0.6669, 0.4480, 0.4202],\n         [0.8847, 0.0659, 0.8775],\n         [0.7223, 0.6830, 0.1865]],\n\n        [[0.8608, 0.6474, 0.1776],\n         [0.1161, 0.5399, 0.1326],\n         [0.9023, 0.2389, 0.9970],\n         [0.4701, 0.8738, 0.8882],\n         [0.3988, 0.3489, 0.0029]]], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "# scalar/1 element\n",
    "x = torch.rand(1, dtype=torch.double)\n",
    "print(\"scalar: \", x)\n",
    "# vector\n",
    "x = torch.rand(3, dtype=torch.double)\n",
    "print(\"vector: \", x)\n",
    "# matrix\n",
    "x = torch.rand(3, 5, dtype=torch.double)\n",
    "print(\"matrix: \", x)\n",
    "# tensor\n",
    "x = torch.rand(3, 5, 3, dtype=torch.double)\n",
    "print(\"tensor: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([3, 5, 3])\n"
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank, Axis and Shape of a tensor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a tensor refers to the number of dimensions present within the tensor. Suppose we are told that we have a rank-2 tensor. This means all of the following:\n",
    "\n",
    "We have a matrix\n",
    "We have a 2d-array\n",
    "We have a 2d-tensor\n",
    "\n",
    "> The rank of a tensor tells us how many indexes are required to access (refer to) a specific data element contained within the tensor data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axes of a tensor\n",
    "\n",
    "If we have a tensor, and we want to refer to a specific dimension, we use the word axis in deep learning.\n",
    "\n",
    "> An axis of a tensor is a specific dimension of a tensor.\n",
    "\n",
    "If we say that a tensor is a rank 2 tensor, we mean that the tensor has 2 dimensions, or equivalently, the tensor has two axes.\n",
    "\n",
    "> Elements are said to exist or run along an axis. \n",
    "\n",
    "This *running* is constrained by the length of each axis. The length of each axis tells us how many indexes are available along each axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the length of an axis now. Suppose we have a tensor called *t*, and its defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n"
    }
   ],
   "source": [
    "t = [\n",
    "[1, 2, 3, 4],\n",
    "[5, 6, 7, 8],\n",
    "[9, 10, 11, 12]\n",
    "]\n",
    "print(np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "shape:  [3 4]\nrank:  2\nnumber of elements:  12\n"
    }
   ],
   "source": [
    "# size/shape\n",
    "shape = np.array(t.size())\n",
    "print(\"shape: \", shape)\n",
    "\n",
    "# rank \n",
    "rank = len(t.shape)\n",
    "print(\"rank: \", rank)\n",
    "\n",
    "# number of elements\n",
    "numel = t.numel()\n",
    "print(\"number of elements: \", numel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the first axis has a length of three while the second axis has a length of four.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "t[0][0]:  1\nt[0][1]:  2\nt[0][2]:  3\nt[0][3]:  4\nt[1][0]:  5\nt[1][1]:  6\nt[1][2]:  7\nt[1][3]:  8\nt[2][0]:  9\nt[2][1]:  10\nt[2][2]:  11\nt[2][3]:  12\n"
    }
   ],
   "source": [
    "print(\"t[0][0]: \", t[0][0])\n",
    "print(\"t[0][1]: \", t[0][1])\n",
    "print(\"t[0][2]: \", t[0][2])\n",
    "print(\"t[0][3]: \", t[0][3])\n",
    "\n",
    "print(\"t[1][0]: \", t[1][0])\n",
    "print(\"t[1][1]: \", t[1][1])\n",
    "print(\"t[1][2]: \", t[1][2])\n",
    "print(\"t[1][3]: \", t[1][3])\n",
    "\n",
    "print(\"t[2][0]: \", t[2][0])\n",
    "print(\"t[2][1]: \", t[2][1])\n",
    "print(\"t[2][2]: \", t[2][2])\n",
    "print(\"t[2][3]: \", t[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each element along the first axis, is an array:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "t[0]:  [1, 2, 3, 4]\nt[1]:  [5, 6, 7, 8]\nt[2]:  [9, 10, 11, 12]\n"
    }
   ],
   "source": [
    "print(\"t[0]: \", t[0])\n",
    "print(\"t[1]: \", t[1])\n",
    "print(\"t[2]: \", t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element along the second axis, is a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "t[0][0]:  1\nt[0][1]:  2\nt[0][2]:  3\nt[0][3]:  4\n"
    }
   ],
   "source": [
    "print(\"t[0][0]: \", t[0][0])\n",
    "print(\"t[0][1]: \", t[0][1])\n",
    "print(\"t[0][2]: \", t[0][2])\n",
    "print(\"t[0][3]: \", t[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, with tensors, the elements of the last axis are always scalar. Every other axis will contain n-dimensional arrays. This is what we see in this example, but this idea generalises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of a tensor\n",
    "\n",
    "The shape of a tensor is determined by the length of each axis, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider the same tensor t as before. To work with this tensor's shape, we’ll create a `torch.Tensor` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n"
    },
    {
     "data": {
      "text/plain": "torch.Size([3, 4])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(t)\n",
    "type(t)\n",
    "print(t)\n",
    "t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a tensor is important for a few reasons. The first reason is because the shape allows us to conceptually think about, or even visualize, a tensor. Higher rank tensors become more abstract, and the shape gives us something concrete to think about.\n",
    "\n",
    "Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called **reshaping**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following high-level categories of operations:\n",
    "\n",
    "* Element-wise operations\n",
    "* Reshaping operations\n",
    "* Reduction operations\n",
    "* Access operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "x:  tensor([[0.9346, 0.3776],\n        [0.7256, 0.0598]])\ny:  tensor([[0.0562, 0.9949],\n        [0.3113, 0.4844]])\naddition:  tensor([[0.9908, 1.3725],\n        [1.0369, 0.5442]])\nsubtraction:  tensor([[ 0.8784, -0.6173],\n        [ 0.4143, -0.4245]])\nmultiplication:  tensor([[0.0525, 0.3757],\n        [0.2259, 0.0290]])\ndivision:  tensor([[16.6254,  0.3795],\n        [ 2.3308,  0.1235]])\n"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(\"x: \", x)\n",
    "print(\"y: \", y)\n",
    "# addition\n",
    "a = torch.add(x, y)\n",
    "print(\"addition: \", a)\n",
    "# subtraction\n",
    "s = torch.sub(x, y)\n",
    "print(\"subtraction: \", s)\n",
    "# multiplication\n",
    "m = torch.mul(x, y)\n",
    "print(\"multiplication: \", m)\n",
    "# division\n",
    "d = torch.div(x, y)\n",
    "print(\"division: \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n t.reshape(1,12):  tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n\n t.reshape(2,6):  tensor([[ 1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12]])\n\n t.reshape(3,4):  tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n\n t.reshape(4,3):  tensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]])\n"
    }
   ],
   "source": [
    "# tensor reshaping example\n",
    "print(\"\\n t.reshape(1,12): \", t.reshape(1,12))\n",
    "print(\"\\n t.reshape(2,6): \", t.reshape(2,6))\n",
    "print(\"\\n t.reshape(3,4): \", t.reshape(3,4))\n",
    "print(\"\\n t.reshape(4,3): \", t.reshape(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one thing to notice about reshaping is that the product of the component values in the shape must equal the total number of elements in the tensor.\n",
    "\n",
    "This makes it so that there are enough positions inside the tensor data structure to contain all of the original data elements after the reshaping.\n",
    "\n",
    "> Reshaping changes the shape but not the underlying data elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changing shape by `squeezing` and `unsqueezing` a tensor\n",
    "\n",
    "* squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "* unsqueezing a tensor adds a dimension with a length of one.\n",
    "\n",
    "These operations are used to expand or shrink the rank (number of dimensions) of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 12])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([12])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 12])\n"
    }
   ],
   "source": [
    "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a common use case for squeezing a tensor by building a flatten function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see that flatten operations are required when passing an output tensor from a convolutional layer to a linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1, 2],\n        [3, 4]])\n"
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[5, 6],\n        [7, 8]])\n"
    }
   ],
   "source": [
    "t2 = torch.tensor([\n",
    "    [5,6],\n",
    "    [7,8]\n",
    "])\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can combine t1 and t2 row-wise (axis-0) in the following way:\n",
    "torch.cat((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 5, 6],\n        [3, 4, 7, 8]])"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can combine them column-wise (axis-1) like this:\n",
    "torch.cat((t1, t2), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch <-> Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([0.6033, 0.6615, 0.2307, 0.4885, 0.4076])\n"
    }
   ],
   "source": [
    "a = torch.rand(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0.60334    0.66145104 0.23070425 0.4885323  0.40763372]\n"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1.6033, 1.6615, 1.2307, 1.4885, 1.4076])\n[1.60334   1.6614511 1.2307043 1.4885323 1.4076338]\n"
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be moved onto any device using the `.to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "- Twitter: https://twitter.com/PyTorch\n",
    "- Forum: https://discuss.pytorch.org/\n",
    "- Tutorials: https://pytorch.org/tutorials/\n",
    "- Examples: https://github.com/pytorch/examples\n",
    "- API Reference: https://pytorch.org/docs/stable/index.html\n",
    "- Torchvision: https://pytorch.org/docs/stable/torchvision/index.html\n",
    "- PyTorch Text: https://github.com/pytorch/text\n",
    "- PyTorch Audio: https://github.com/pytorch/audio\n",
    "- AllenNLP: https://allennlp.org/\n",
    "- Object detection/segmentation: https://github.com/facebookresearch/maskrcnn-benchmark\n",
    "- Facebook AI Research Sequence-to-Sequence Toolkit written in PyTorch: https://github.com/pytorch/fairseq\n",
    "- FastAI http://www.fast.ai/\n",
    "- Stanford CS230 Deep Learning notes https://cs230-stanford.github.io"
   ]
  }
 ]
}